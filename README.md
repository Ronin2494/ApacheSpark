# ApacheSpark

Analyze data using PySpark, the original setup is done in Microsoft Fabric. 

First, a pipeline is created to ingest a dataset and then executed to load the data in a lakehouse.

To analyze the data, Pyspark and SparkSQL are used. 

The data is a sample dataset available in a Data Lake, this is just for practice purposes. 

If you need help on how to ingest a dataframe: Check out Microsoft's Official tutorials, they are well-built and cover all concepts, or use the below link.

[UseApacheSpark](https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse)



